{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_356552/2481555487.py:4: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Files\n",
    "\n",
    "from transformers import TextStreamer\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bf16_supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.14: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    NVIDIA RTX A4000. Num GPUs = 1. Max memory: 14.728 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Load local model\n",
    "\n",
    "path = \"Llama1B\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=path, \n",
    "    load_in_4bit=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9000c51d5f246b791ed7daad892ea3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24783 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset: CSV --> Pandas --> Dataset\n",
    "\n",
    "# Load CSV --> Pandas\n",
    "dataset_path = \"dataset/labeled_data.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Pandas to Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def format_tweets(data): \n",
    "    tweets = data['tweet']\n",
    "    texts = []\n",
    "    for tweet in tweets: \n",
    "        text = tweet + \"_\" + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "\n",
    "    return { \"text\": texts, }\n",
    "\n",
    "formatted_dataset = dataset.map(format_tweets, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"!!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp; as a man you should always take the trash out..._<|end_of_text|>\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the formatted dataset\n",
    "\n",
    "formatted_dataset['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcamel000000\u001b[0m (\u001b[33mcamel000000-connecticut-college\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/research/Desktop/ammerman/llama1B/wandb/run-20250315_163123-i7pm5c39</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/camel000000-connecticut-college/Initial%20Test%20from%20Local%20Dataset/runs/i7pm5c39?apiKey=559d703ad5ea1855ef9d92f421c85719a8c8d4cb' target=\"_blank\">swift-surf-4</a></strong> to <a href='https://wandb.ai/camel000000-connecticut-college/Initial%20Test%20from%20Local%20Dataset?apiKey=559d703ad5ea1855ef9d92f421c85719a8c8d4cb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/camel000000-connecticut-college/Initial%20Test%20from%20Local%20Dataset?apiKey=559d703ad5ea1855ef9d92f421c85719a8c8d4cb' target=\"_blank\">https://wandb.ai/camel000000-connecticut-college/Initial%20Test%20from%20Local%20Dataset?apiKey=559d703ad5ea1855ef9d92f421c85719a8c8d4cb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/camel000000-connecticut-college/Initial%20Test%20from%20Local%20Dataset/runs/i7pm5c39?apiKey=559d703ad5ea1855ef9d92f421c85719a8c8d4cb' target=\"_blank\">https://wandb.ai/camel000000-connecticut-college/Initial%20Test%20from%20Local%20Dataset/runs/i7pm5c39?apiKey=559d703ad5ea1855ef9d92f421c85719a8c8d4cb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use Weight And Biases for Callbacks\n",
    "run = wandb.init(\n",
    "    project=\"Initial Test from Local Dataset\", \n",
    "    job_type=\"Training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.14 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Set up the training\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model, \n",
    "    r=16, \n",
    "    target_modules= [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ], \n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0, \n",
    "    bias=\"none\", \n",
    "   \n",
    "    use_gradient_checkpointing=\"unsloth\", \n",
    "    random_state=3407,\n",
    "    use_rslora=False, \n",
    "    loftq_config=None, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28c195bebe2463eb76233283a6e8c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/24783 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_seq_length = 2048\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model, \n",
    "    tokenizer = tokenizer, \n",
    "    train_dataset = formatted_dataset, \n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2, # ???\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 64,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 10,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 24,783 | Num Epochs = 1 | Total steps = 10\n",
      "O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 4 x 1) = 256\n",
      " \"-____-\"     Trainable parameters = 11,272,192/1,000,000,000 (1.13% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:27, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.938700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.723100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.602400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I think woman are so stupid.']\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "    [\"I think woman\"],\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids = inputs.input_ids, \n",
    "    attention_mask = inputs.attention_mask, \n",
    "    max_new_tokens=250, \n",
    "    use_cache= True, \n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('out/test01/tokenizer_config.json',\n",
       " 'out/test01/special_tokens_map.json',\n",
       " 'out/test01/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model\n",
    "\n",
    "import os\n",
    "\n",
    "out_path = os.path.join(\"out\", \"test01\")\n",
    "\n",
    "model.save_pretrained(out_path)\n",
    "tokenizer.save_pretrained(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.14: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    NVIDIA RTX A4000. Num GPUs = 1. Max memory: 14.728 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Load local model\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=out_path, \n",
    "    load_in_4bit=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I think woman are to stupid to know how to take care of a man.\\nI am a girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and I want to be a good girl and']\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "    [\"I think woman\"],\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids = inputs.input_ids, \n",
    "    attention_mask = inputs.attention_mask, \n",
    "    max_new_tokens=250, \n",
    "    use_cache= True, \n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "make: *** No rule to make target 'clean'.  Stop.\n",
      "make: *** No rule to make target 'all'.  Stop.\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 10.99 out of 31.06 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 76.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n",
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['f16'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsloth: The file ('llama.cpp/llama-quantize' or 'llama.cpp/llama-quantize.exe' if you are on Windows WSL) or 'llama.cpp/quantize' does not exist.\nBut we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake all -j\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgit log -1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained_gguf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/unsloth/save.py:1856\u001b[0m, in \u001b[0;36munsloth_save_pretrained_gguf\u001b[0;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1853\u001b[0m is_sentencepiece_model \u001b[38;5;241m=\u001b[39m check_if_sentencepiece_model(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;66;03m# Save to GGUF\u001b[39;00m\n\u001b[0;32m-> 1856\u001b[0m all_file_locations, want_full_precision \u001b[38;5;241m=\u001b[39m \u001b[43msave_to_gguf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_sentencepiece_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_save_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_conversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmakefile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;66;03m# Save Ollama modelfile\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m modelfile \u001b[38;5;241m=\u001b[39m create_ollama_modelfile(tokenizer, all_file_locations[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/unsloth/save.py:1085\u001b[0m, in \u001b[0;36msave_to_gguf\u001b[0;34m(model_type, model_dtype, is_sentencepiece, model_directory, quantization_method, first_conversion, _run_installer)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     quantize_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama.cpp/llama-quantize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: The file (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama.cpp/llama-quantize\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama.cpp/llama-quantize.exe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m if you are on Windows WSL) or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama.cpp/quantize\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1087\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1088\u001b[0m     )\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# See https://github.com/unslothai/unsloth/pull/730\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;66;03m# Filenames changed again!\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsloth: The file ('llama.cpp/llama-quantize' or 'llama.cpp/llama-quantize.exe' if you are on Windows WSL) or 'llama.cpp/quantize' does not exist.\nBut we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"f16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 9.68 out of 31.06 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 77.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
